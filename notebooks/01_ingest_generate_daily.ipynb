{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "700f2a79-ed15-4f9b-b59f-1178886bcaeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## %pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fef3f36e-b0d2-4a1d-9193-ac908a04c669",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook: 01_ingest_generate_daily.ipynb\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Initialize\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "fake = Faker()\n",
    "np.random.seed(88)\n",
    "\n",
    "# Parameters\n",
    "NUM_RECORDS = np.random.randint(400, 600)\n",
    "TODAY = datetime.today().date() - timedelta(days=1)  # yesterday for daily batch\n",
    "\n",
    "# Static Data\n",
    "companies = ['Spendora', 'Expensivus', 'ClarityLedger', 'TrueSpend', 'Fintrix', 'Procuro', 'LedgrIQ', 'Zentro']\n",
    "departments = ['Engineering', 'Marketing', 'Sales', 'Finance', 'HR', 'Operations']\n",
    "categories = ['Travel', 'Meals', 'Supplies', 'Entertainment', 'Misc']\n",
    "merchants = {\n",
    "    'Travel': ['Delta', 'Uber', 'Lyft', 'Marriott', 'Hilton'],\n",
    "    'Meals': ['Starbucks', 'Chipotle', 'Panera', 'Olive Garden'],\n",
    "    'Supplies': ['Staples', 'Office Depot', 'Amazon'],\n",
    "    'Entertainment': ['AMC', 'TopGolf', \"Dave & Buster's\"],\n",
    "    'Misc': ['Etsy', 'Other', 'Unknown']\n",
    "}\n",
    "category_amounts = {\n",
    "    'Travel': (500, 150),\n",
    "    'Meals': (40, 10),\n",
    "    'Supplies': (50, 20),\n",
    "    'Entertainment': (100, 50),\n",
    "    'Misc': (75, 75)\n",
    "}\n",
    "\n",
    "# Generate synthetic data\n",
    "data = []\n",
    "for _ in range(NUM_RECORDS):\n",
    "    company = random.choice(companies)\n",
    "    department = random.choice(departments)\n",
    "    category = random.choice(categories)\n",
    "    merchant = random.choice(merchants[category])\n",
    "    mean, std = category_amounts[category]\n",
    "    amount = round(max(1, np.random.normal(loc=mean, scale=std)), 2)\n",
    "    employee = fake.name()\n",
    "\n",
    "    data.append([\n",
    "        employee, company, department, category,\n",
    "        merchant, amount, str(TODAY), 'transaction'\n",
    "    ])\n",
    "\n",
    "# Create DataFrame\n",
    "df_pd = pd.DataFrame(data, columns=[\n",
    "    'employee', 'company', 'department', 'category',\n",
    "    'merchant', 'amount', 'date', 'type'\n",
    "])\n",
    "df_spark = spark.createDataFrame(df_pd)\n",
    "\n",
    "# Write to Bronze Delta table path\n",
    "bronze_path = \"dbfs:/mnt/your_mount_point/bronze/daily_transactions\"\n",
    "df_spark.write.format(\"delta\").mode(\"append\").save(bronze_path)\n",
    "\n",
    "print(f\"âœ… {NUM_RECORDS} daily transactions written to {bronze_path} for {TODAY}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_ingest_generate_daily",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
